{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:/TwitterDataCollection/hao_CSA_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every folder, concatenate all the csv files under the same header, add the column for the query that was used to get it(filename), and then remove duplicate lines from the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = [ f.name for f in os.scandir(file_path) if f.is_dir() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abortion', 'climate_change', 'covid', 'gun_control', 'immigration', 'racism']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal of duplicates 2576\n",
      "After removal of duplicates 2019\n",
      "completed  abortion\n",
      "Before removal of duplicates 2634\n",
      "After removal of duplicates 2334\n",
      "completed  climate_change\n",
      "Before removal of duplicates 400\n",
      "After removal of duplicates 371\n",
      "completed  covid\n",
      "Before removal of duplicates 2475\n",
      "After removal of duplicates 1796\n",
      "completed  gun_control\n",
      "Before removal of duplicates 72\n",
      "After removal of duplicates 60\n",
      "completed  immigration\n",
      "Before removal of duplicates 3856\n",
      "After removal of duplicates 3585\n",
      "completed  racism\n"
     ]
    }
   ],
   "source": [
    "for folder in subfolders:\n",
    "    sub_file_path = file_path+folder+\"/\"\n",
    "    \n",
    "    os.chdir(sub_file_path)\n",
    "    all_files = glob.glob(os.path.join(sub_file_path, \"*.csv\"))\n",
    "  \n",
    "    all_df = []\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        query_file = f.split('\\\\')[-1]\n",
    "        df['file'] = query_file[:-4]\n",
    "        all_df.append(df)\n",
    "    merged_df = pd.concat(all_df, ignore_index=True, sort=True)\n",
    "    print(\"Before removal of duplicates\", len(merged_df))\n",
    "    merged_df = merged_df.drop_duplicates(subset='Embedded_text', keep=\"first\")\n",
    "    print(\"After removal of duplicates\", len(merged_df))\n",
    "    merged_df.to_csv(file_path+\"combined_\"+folder+\".csv\")\n",
    "    print(\"completed \",folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterDataCollection",
   "language": "python",
   "name": "twitterdatacollection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
